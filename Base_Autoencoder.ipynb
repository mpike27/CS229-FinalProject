{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv3D, Conv2DTranspose, Conv3DTranspose, Flatten, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import Hyperparameters as hyp\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Data/Training2/'\n",
    "test_path = '../input/maxcs229test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_opts = [0.001, 0.0001]\n",
    "decay_opts = [0.0001, 0.00001]\n",
    "eps_opts = [0.00001, 0.000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------- SlidingWindow.py ---------------------------#\n",
    "def get_sliding_windows(frames_list, sequence_size):\n",
    "    \"\"\" For data augmenting purposes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    frames_list : list\n",
    "        A list of sorted frames of shape Config.FRAME_RES X Config.FRAME_RES\n",
    "    sequence_size: int\n",
    "        The size of the lstm sequence\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of clips , 20 frames each\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    for i in range(len(frames_list) - sequence_size):\n",
    "        clip = np.zeros(shape=(sequence_size, hyp.FRAME_RES, hyp.FRAME_RES, 1))\n",
    "        clip[:, :, :, 0] = frames_list[i : i + sequence_size]\n",
    "        clips.append(clip)\n",
    "    return np.array(clips)\n",
    "\n",
    "def getData(path):\n",
    "    data = []\n",
    "    for file in os.listdir(path):\n",
    "        if file[-3:] == \"npy\":\n",
    "            all_frames = np.load(path + file, allow_pickle=True)\n",
    "            data.extend(get_sliding_windows(frames_list=all_frames, sequence_size=hyp.CLIP_LEN))\n",
    "            \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment1(train_flat, val_flat):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(train_flat.shape[1] / 2), activation=\"relu\", input_shape=(train_flat.shape[1],)))\n",
    "    model.add(Dense(train_flat.shape[1]))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=hyp.lr['Base_Auto'], decay=hyp.decay['Base_Auto'], epsilon=hyp.eps['Base_Auto']), loss=\"mse\")\n",
    "    model.summary()\n",
    "    es = EarlyStopping(min_delta=hyp.eps['Base_Auto'], verbose=1, patience=3)\n",
    "    history = model.fit(train_flat, train_flat, validation_data=(val_flat, val_flat), epochs=10000, callbacks=[es])\n",
    "    model.save(\"Base_Auto\")\n",
    "    np.save(\"Base_Loss\", np.array([history.history['val_loss'], history.history['loss']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data shape is (40583, 10, 32, 32, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5120)              52433920  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10240)             52439040  \n",
      "=================================================================\n",
      "Total params: 104,872,960\n",
      "Trainable params: 104,872,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40583 samples, validate on 4510 samples\n",
      "Epoch 1/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 1500.9742 - val_loss: 1232.5187\n",
      "Epoch 2/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 1151.3873 - val_loss: 1086.4935\n",
      "Epoch 3/10000\n",
      "40583/40583 [==============================] - 58s 1ms/sample - loss: 1040.6515 - val_loss: 1007.6048\n",
      "Epoch 4/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 970.2353 - val_loss: 955.3782\n",
      "Epoch 5/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 921.4051 - val_loss: 917.6821\n",
      "Epoch 6/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 885.0529 - val_loss: 886.5319\n",
      "Epoch 7/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 856.6942 - val_loss: 862.6203\n",
      "Epoch 8/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 834.1362 - val_loss: 841.7514\n",
      "Epoch 9/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 815.6839 - val_loss: 826.3317\n",
      "Epoch 10/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 800.2437 - val_loss: 814.7430\n",
      "Epoch 11/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 787.1164 - val_loss: 804.0267\n",
      "Epoch 12/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 775.9979 - val_loss: 794.4160\n",
      "Epoch 13/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 766.2155 - val_loss: 784.8546\n",
      "Epoch 14/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 757.7607 - val_loss: 778.4656\n",
      "Epoch 15/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 750.2636 - val_loss: 771.5669\n",
      "Epoch 16/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 743.6077 - val_loss: 765.9838\n",
      "Epoch 17/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 737.6579 - val_loss: 761.4911\n",
      "Epoch 18/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 732.3885 - val_loss: 756.1015\n",
      "Epoch 19/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 727.6366 - val_loss: 751.9171\n",
      "Epoch 20/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 723.2364 - val_loss: 748.2527\n",
      "Epoch 21/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 719.3309 - val_loss: 745.1391\n",
      "Epoch 22/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 715.7801 - val_loss: 741.9830\n",
      "Epoch 23/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 712.4995 - val_loss: 738.9029\n",
      "Epoch 24/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 709.5238 - val_loss: 736.5428\n",
      "Epoch 25/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 706.7632 - val_loss: 733.4697\n",
      "Epoch 26/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 704.2229 - val_loss: 731.2240\n",
      "Epoch 27/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 701.8592 - val_loss: 729.2722\n",
      "Epoch 28/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 699.6406 - val_loss: 727.7841\n",
      "Epoch 29/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 697.6432 - val_loss: 725.4484\n",
      "Epoch 30/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 695.6862 - val_loss: 723.3618\n",
      "Epoch 31/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 693.8957 - val_loss: 723.6054\n",
      "Epoch 32/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 692.1994 - val_loss: 720.8511\n",
      "Epoch 33/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 690.6532 - val_loss: 719.5890\n",
      "Epoch 34/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 689.1224 - val_loss: 717.8061\n",
      "Epoch 35/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 687.7237 - val_loss: 716.6598\n",
      "Epoch 36/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 686.3618 - val_loss: 714.8966\n",
      "Epoch 37/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 685.1129 - val_loss: 714.2143\n",
      "Epoch 38/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 683.9073 - val_loss: 713.5612\n",
      "Epoch 39/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 682.7995 - val_loss: 711.9383\n",
      "Epoch 40/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 681.6931 - val_loss: 710.6409\n",
      "Epoch 41/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 680.6772 - val_loss: 710.4277\n",
      "Epoch 42/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 679.6699 - val_loss: 709.2958\n",
      "Epoch 43/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 678.7452 - val_loss: 708.4338\n",
      "Epoch 44/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 677.8370 - val_loss: 707.7208\n",
      "Epoch 45/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 677.0001 - val_loss: 707.1668\n",
      "Epoch 46/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 676.1625 - val_loss: 705.8766\n",
      "Epoch 47/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 675.3733 - val_loss: 705.4355\n",
      "Epoch 48/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 674.6261 - val_loss: 704.5912\n",
      "Epoch 49/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 673.8993 - val_loss: 704.0297\n",
      "Epoch 50/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 673.1956 - val_loss: 703.0592\n",
      "Epoch 51/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 672.4947 - val_loss: 702.5210\n",
      "Epoch 52/10000\n",
      "40583/40583 [==============================] - 65s 2ms/sample - loss: 671.8595 - val_loss: 702.0199\n",
      "Epoch 53/10000\n",
      "40583/40583 [==============================] - 64s 2ms/sample - loss: 671.2322 - val_loss: 701.1913\n",
      "Epoch 54/10000\n",
      "40583/40583 [==============================] - 64s 2ms/sample - loss: 670.6251 - val_loss: 700.5724\n",
      "Epoch 55/10000\n",
      "40583/40583 [==============================] - 60s 1ms/sample - loss: 670.0286 - val_loss: 700.1686\n",
      "Epoch 56/10000\n",
      "40583/40583 [==============================] - 58s 1ms/sample - loss: 669.4522 - val_loss: 700.0915\n",
      "Epoch 57/10000\n",
      "40583/40583 [==============================] - 64s 2ms/sample - loss: 668.9024 - val_loss: 699.2981\n",
      "Epoch 58/10000\n",
      "40583/40583 [==============================] - 64s 2ms/sample - loss: 668.3674 - val_loss: 698.7831\n",
      "Epoch 59/10000\n",
      "40583/40583 [==============================] - 64s 2ms/sample - loss: 667.8547 - val_loss: 698.2817\n",
      "Epoch 60/10000\n",
      "40583/40583 [==============================] - 59s 1ms/sample - loss: 667.3696 - val_loss: 697.8117\n",
      "Epoch 61/10000\n",
      "40583/40583 [==============================] - 63s 2ms/sample - loss: 666.8731 - val_loss: 697.2451\n",
      "Epoch 62/10000\n",
      "40583/40583 [==============================] - 64s 2ms/sample - loss: 666.4012 - val_loss: 696.9393\n",
      "Epoch 63/10000\n",
      "40583/40583 [==============================] - 63s 2ms/sample - loss: 665.9492 - val_loss: 696.5490\n",
      "Epoch 64/10000\n",
      "40583/40583 [==============================] - 60s 1ms/sample - loss: 665.5112 - val_loss: 696.1617\n",
      "Epoch 65/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 665.0731 - val_loss: 695.7340\n",
      "Epoch 66/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 664.6652 - val_loss: 695.2324\n",
      "Epoch 67/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 664.2679 - val_loss: 695.0576\n",
      "Epoch 68/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 663.8623 - val_loss: 694.4326\n",
      "Epoch 69/10000\n",
      "40583/40583 [==============================] - 57s 1ms/sample - loss: 663.4827 - val_loss: 694.3816\n",
      "Epoch 70/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 663.1106 - val_loss: 693.8052\n",
      "Epoch 71/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 662.7266 - val_loss: 693.3746\n",
      "Epoch 72/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 662.3757 - val_loss: 693.4201\n",
      "Epoch 73/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 662.0225 - val_loss: 692.7897\n",
      "Epoch 74/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 661.6965 - val_loss: 692.6041\n",
      "Epoch 75/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 661.3612 - val_loss: 692.2003\n",
      "Epoch 76/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 661.0373 - val_loss: 691.8348\n",
      "Epoch 77/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 660.7169 - val_loss: 691.6310\n",
      "Epoch 78/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 660.4059 - val_loss: 691.1599\n",
      "Epoch 79/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 660.1046 - val_loss: 691.0012\n",
      "Epoch 80/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 659.8274 - val_loss: 690.8110\n",
      "Epoch 81/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 659.5264 - val_loss: 690.3886\n",
      "Epoch 82/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 659.2477 - val_loss: 690.1700\n",
      "Epoch 83/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 658.9795 - val_loss: 689.8544\n",
      "Epoch 84/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 658.7142 - val_loss: 689.6165\n",
      "Epoch 85/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 658.4504 - val_loss: 689.5269\n",
      "Epoch 86/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 658.2104 - val_loss: 689.5077\n",
      "Epoch 87/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 657.9548 - val_loss: 688.9919\n",
      "Epoch 88/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 657.7157 - val_loss: 688.6374\n",
      "Epoch 89/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 657.4797 - val_loss: 688.4746\n",
      "Epoch 90/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 657.2452 - val_loss: 688.1484\n",
      "Epoch 91/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 657.0201 - val_loss: 688.0492\n",
      "Epoch 92/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 656.8122 - val_loss: 687.7110\n",
      "Epoch 93/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 656.5809 - val_loss: 687.5604\n",
      "Epoch 94/10000\n",
      "40583/40583 [==============================] - 56s 1ms/sample - loss: 656.3777 - val_loss: 687.3709\n",
      "Epoch 95/10000\n",
      "35232/40583 [=========================>....] - ETA: 7s - loss: 655.5407"
     ]
    }
   ],
   "source": [
    "train_data = getData(train_path)\n",
    "train_data, val_data, _,  _ = train_test_split(train_data, train_data, test_size=0.1)\n",
    "train_flat = np.array([np.ravel(train_data[i]) for i in range(train_data.shape[0])])\n",
    "val_flat = np.array([np.ravel(val_data[i]) for i in range(val_data.shape[0])])\n",
    "print(f\"Train_data shape is {train_data.shape}\")\n",
    "experiment1(train_flat, val_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "val_loss = sys.maxsize\n",
    "best_lr, best_decay, best_eps = 0, 0, 0\n",
    "for lr in lr_opts:\n",
    "    for decay in decay_opts:\n",
    "        for eps in eps_opts:          \n",
    "            model = Sequential()\n",
    "            model.add(Dense(int(train_flat.shape[1] / 2), activation=\"relu\", input_shape=(train_flat.shape[1],)))\n",
    "            model.add(Dense(train_flat.shape[1]))\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr, decay=decay, epsilon=eps), loss=\"mse\")\n",
    "            model.summary()\n",
    "            es = EarlyStopping(min_delta=eps, verbose=1, patience=3)\n",
    "            history = model.fit(train_flat, train_flat, validation_data=(val_flat, val_flat), epochs=10000, callbacks=[es])\n",
    "            val = history.history['val_loss']\n",
    "            test = history.history['loss']\n",
    "            if val[-1] < val_loss:\n",
    "                val_loss = val[-1]\n",
    "                best_lr = lr\n",
    "                best_decay = decay\n",
    "                best_eps = eps\n",
    "                model.save(\"Base_Auto\")\n",
    "                np.save(\"Base_Loss\", np.array([val, test]))\n",
    "print('The optimal training hyperparams are:')\n",
    "print(f\"lr = {best_lr}\")\n",
    "print(f\"decay = {best_decay}\")\n",
    "print(f\"eps = {best_eps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
